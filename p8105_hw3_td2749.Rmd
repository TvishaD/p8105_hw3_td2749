---
title: "p8105_hw3_td2749"
author: "Tvisha R. Devavarapu"
date: "2022-10-07"
output: github_document
---

```{r Setting up, message = FALSE}
library(tidyverse)
library(ggplot2)
library(ggridges)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 10,
  fig.asp = .6,
  out.width = "90%"
)
```


## Problem 1: Instacart Data


```{r Setting up instacart}
library(p8105.datasets)
data("instacart")

instacart_df =
  instacart %>%
    janitor::clean_names()
```

**Task 1**: The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and giving illustrative examples of observations. 

_**Description**_: The data consists of `r nrow(instacart)` rows of observations and `r ncol(instacart)` columns of variables. The column names are: `r colnames(instacart)`. `order_id` and `user_id` are related as rows with the same order_id reveal the items ordered by an individual with the respective user_id. It has been given that there are 131,209 unique users and that there is a single order per user in this dataset. Each row is a product from an order. For instance, row 9 has: order_id = 36, product_id = 39612, add_to_cart_order = 1 (indicating it was the first item to be added to the cart in this session), reordered = 0 (indicating that this item has not been ordered by this user in the past), user_id = 79431 (identifying the user), eval_set = train, order_number = 23 (indicating this user's order sequence number), order_dow = 6 (indicating day on which the order was placed), order_hour_of_day = 18, days_since_prior_order = 30, product_name = Grated Pecorino Romano Cheese, aisle_id = 2, department_id = 16, aisle = specialty cheeses, and department = dairy eggs. 


**Task 1.1**: How many aisles are there, and which aisles are the most items ordered from?

```{r number of aisles and most-ordered}
n_distinct(instacart_df$aisle_id)

instacart_df %>% 
  count(aisle) %>% 
  arrange(desc(n))
```

There are 134 aisles. Fresh vegetables (150609), and fresh fruits (150473) seem to be the aisles from which most items were ordered in this dataset. 



**Task 1.2**: Make a plot that shows the number of items ordered in each aisle, limiting this to aisles with more than 10000 items ordered. Arrange aisles sensibly, and organize your plot so others can read it.

```{r aisle plot}
instacart_df %>% 
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(aisle = fct_reorder(aisle, n)) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() + 
  labs(title = "Number of items ordered in each aisle") +
  theme(axis.text.x = element_text(angle = 60, hjust = 1))
```


**Task 1.3**: Make a table showing the three most popular items in each of the aisles “baking ingredients”, “dog food care”, and “packaged vegetables fruits”. Include the number of times each item is ordered in your table.

```{r popular items}
instacart_df %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(desc(n)) %>%
  knitr::kable()
```


**Task 1.4**: Make a table showing the mean hour of the day at which Pink Lady Apples and Coffee Ice Cream are ordered on each day of the week; format this table for human readers (i.e. produce a 2 x 7 table).

```{r}
instacart_df %>%
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>%
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(names_from = order_dow, values_from = mean_hour) %>% 
  knitr::kable()
```


## Problem 2: Accelerometer Data


**Task 2.1**: Load, tidy, and otherwise wrangle the data. Your final dataset should include all originally observed variables and values; have useful variable names; include a weekday vs weekend variable; and encode data with reasonable variable classes. Describe the resulting dataset (e.g. what variables exist, how many observations, etc).

```{r accel_data set up}
accel_data = 
  read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  mutate(day_type = ifelse(day %in% c("Saturday","Sunday"), "weekend", "weekday")) %>% 
  mutate(day = fct_relevel(day, "Monday", "Tuesday", "Wednesday", "Thursday",
                                  "Friday", "Saturday", "Sunday")) %>%
  pivot_longer(activity_1:activity_1440, names_to = "minute_of_day",
               values_to = "activity_count") %>% 
  group_by(week, day) %>%
  mutate(total_activity_count = sum(activity_count)) %>% 
  ungroup() %>% 
  pivot_wider(names_from = minute_of_day, values_from = activity_count) %>% 
  select(week, day_id, day, day_type, total_activity_count, everything())
```

_**Description**_: Without losing any of the originally observed variables and values, the resultant `accel_data` df contains 35 unique rows of observations. Each row represents a day (1-35 days). The final df has 1445 columns of variables: `week` consists of values from 1-5 depicting the respective week of observation, `day_id` and `day` identify the unique days of observation (all 7 days of a week * 5 weeks = 35 days = 35 rows), `day_type` indicates whether a given day is a weekend or a weekday, `total_activity_count` represents the sums across each minute's activity counts over a given day (sum of 1140 (24*60) values), and `activity_1` to `activity_1440` represent the activity counts across each minute. 



**Task 2.2**: Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate across minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?

```{r Total Activity - Table and Graph}
total_activity_table = 
  accel_data %>% 
  group_by(week, day) %>% 
  summarize(total_activity_count) %>% 
  pivot_wider(names_from = day, values_from = total_activity_count) %>% 
  knitr::kable(caption = "Daily Total Activity (as observed counts): Across 5 weeks")

total_activity_table

total_activity_plot = 
  accel_data %>%
    mutate(week = as.factor(week)) %>% 
    ggplot(aes(x = day, y = total_activity_count, color = week, group = week)) +
    geom_point() +
    geom_line() +
    labs(title = "Total Activity Count Vs. Day: Across 5 Weeks",
       x = "Day",
       y = "Total Activity Count") +
    theme(plot.title = element_text(hjust = 0.5, face = "bold"),
          axis.title = element_text(face = "bold"),
          legend.position = "bottom")

total_activity_plot
```

_**Description:**_ From the table and graph generated above, here are some observed trends:

* Compared to other weeks, activity is relatively more consistent throughout week 3.
* Activity on Tuesday seems to be most consistent across all five weeks.
* Generally upward trending activity between Tuesday and Thursday of weeks 1, 2, and 3.
* Activity on Saturday of weeks 4 and 5 seems extremely low, even assuming lower than usual activity. This could be a result of the accelerometer not being worn on these days. 
* Across all weeks, activity was greater on Wednesday than on Tuesday and on Friday than on Thursday (apart from week 4). 
* Apart from week 4, Fridays seem to have consistently high total activities.



**Task 2.3**: Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.

```{r 24 Hour Plot}
over_the_day_plot = 
  accel_data %>%
    pivot_longer(activity_1:activity_1440, names_to = "minute_of_day", 
                 names_prefix = "activity_", values_to = "activity_count") %>% 
    mutate(minute_of_day = as.numeric(minute_of_day)) %>% 
    ggplot(aes(x = minute_of_day, y = activity_count, color = day)) +
      geom_point(alpha = 0.5) +
      scale_x_continuous(
        breaks = c(0, 360, 720, 1080, 1440), 
        labels = c("12 AM","6 AM", "12 PM", "6 PM", "11:59 PM")) +
      labs(title = "Activity Count Vs. Hour",
           x = "Hour",
           y = "Activity Count",
           caption = "Composite: Monday through Sunday; across 5 weeks") +
      theme(plot.title = element_text(hjust = 0.5, face = "bold"),
            plot.caption = element_text(hjust = 0.5),
            axis.title = element_text(face = "bold"),
            legend.position = "bottom") +
      viridis::scale_color_viridis(name = "week", discrete = TRUE)

over_the_day_plot
```
_**Description**_: From the plot above, some patterns/conclusions are:

* Sunday mornings-afternoons (11:00 AM - 1:00 PM) and Friday evenings (9:00 PM) reveal most apparent clusters of activity. 
* Thursday mornings around 7 AM reveals a consistent activity pattern, indicating a possible repeating exercise routine? 
* Clusters of activity on Saturdays (around 5 PM and 8 PM)
* The highest occurring activity on Mondays is usually between 7-9 PM. 
* Tuesdays are relatively the least active days for this individual. 
* Some sparse high activity on Wednesday evenings (7-10 PM), other than which Wednesdays seem pretty sedentary. 



## Problem 3: NY NOAA Data

```{r Setting up ny_noaa}
library(p8105.datasets)
data("ny_noaa")
```

**Task 3**: The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue.


_**Description**_: The original `ny_noaa` dataset consists of `r nrow(ny_noaa)` rows of observations containing weather information recorded in `r length(unique(ny_noaa$id))` unique New York weather stations. Each row depicts readings from a particular day and a particular location. There are `r ncol(ny_noaa)` number of columns: `r ncol(ny_noaa)`. 'id' (character) identifies the station of recording, 'date' (double) indicates the day of observation, 'prcp' (numeric) depicts precipitation in tenths of mm (millimeters), 'snow' (numeric) depicts snowfall in mm, 'snwd' (numeric) depicts snow depth in mm, and 'tmax' (character) and 'tmin' (character) depict the maximum and minimum temperature observed on that day in tenths of degrees Celcius (°C). 

Upon brief inspection of the dataset, it can be concluded that there is quite a lot of missing data. There are some rows where all weather recordings are missing, and some where there are certain combinations missing. An initial broad assumption for this could be that the missing values could reflect: 

a) No rain/snow/snowdepth to measure at a specific location on a certain day. (Weather conditions/seasonal shifts)
b) No temperature and/or snow fall readings taken at certain weather stations

As conditions about the availability of data are not explicit, it is not clear whether the rows with NA values should be completely discarded or imputed with some other values. This might change based on the needs of the questions being answered. If there is a lot of missing data that would otherwise be required to answer a specific/relevant question, this dataset would be considered less effective and the issue of the missing data would be more pertinent.

Note: There are no missing values in the `id` and `date` columns. 


**Task 3.1**: Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?

```{r relatively tidy ny_noaa_df}
ny_noaa_df = 
  ny_noaa %>%
    janitor::clean_names() %>% 
      mutate(date, date = as.character(date)) %>% 
        separate(date, into = c("year", "month", "day"), sep = "-") %>% 
    mutate(
      year = as.numeric(year),
      month = as.numeric(month),
      day = as.numeric(day),
      tmax = as.numeric(tmax),
      tmin = as.numeric(tmin),
      tmax = tmax/10,
      tmin = tmin/10,
      prcp = prcp/10,
      snow = snow/25.4) %>% 
    rename(
      prcp_mm = prcp, 
      snow_inch = snow, 
      snwd_mm = snwd, 
      tmax_c = tmax, 
      tmin_c = tmin)
```

For snowfall, the most commonly observed value is `r tail(names(sort(table(ny_noaa_df$snow_inch))), 1)` inches. There could be several reasons for this occurrence: 

* It only snows during few months of the year (November end to March beginning), explaining the abundance of '0' readings across the other months. 
* Even within those months, it might not be snowing everyday. As snowfall is measured to observe fresh-falling snow on a 24-hr basis, it is reasonable to infer that there may be certain non-snowing days during the winter months. 

The next common occurrences are "0.984251968503937", "0.511811023622047", "2.00787401574803", and "2.99212598425197" (ALL IN INCHES).

_**Description of resultant `ny_noaa_df`**_: The resultant `ny_noaa_df` consists of all the rows as observed in `ny_noaa` (i.e 2595176). Instead of 7 columns, there are now 9 columns of variables, as date has been split into 3 different columns (`year`, `month`, and `day`). These resultant columns contain data in the numeric format. Values in the `tmax` and `tmin` columns have been numerically manipulated via division by 10, resulting in normal Celcius unit observations. These columns were then renamed as `tmax_c` and `tmin_c` respectively. Values in the `snow` column were divided by 25.4 to present the observations in inches (as the snowfall is usually reported in inches and not in millimeters). The resultant column was renamed as `snow_inch`. The `snwd` snow depth column was renamed as `snwd_mm` to reflect the unit of measurement (millimeters).


**Task 3.2**: Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?
```{r ny_noaa avg.tmax jan and july}
avg_tmax_plot = 
  ny_noaa_df %>%
    group_by(id, year, month) %>%
    filter(month %in% c(1, 7)) %>% 
    summarize(mean_monthly_tmax = mean(tmax_c, na.rm = TRUE)) %>% 
    mutate(
      month = as.character(month),
      month = recode(month, "1" = "January", "7" = "July")) %>% 
    drop_na(mean_monthly_tmax) %>% 
    ggplot(aes(x = year, y = mean_monthly_tmax, group = id, color = id)) + 
      geom_point(alpha = .1) +
      geom_line(alpha = .3) + 
      scale_x_continuous(
        breaks = c(1980, 1985, 1990, 1995, 2000, 2005, 2010)) +
      facet_grid(. ~ month) +
      labs(title = "Average Monthly Maximum Temperature (Jan and July) Vs. Year",
         x = "Year",
         y = "Average Maximum Temperature (in °C)",
         caption = "Composite: 1980 through 2010; across unique weather stations") +
      theme(
        legend.position = "none",
        plot.title = element_text(hjust = 0.5, face = "bold"),
        plot.caption = element_text(hjust = 0.5),
        axis.title = element_text(face = "bold"))

avg_tmax_plot
```

_**Description**_: It is extremely fascinating how the max temperature patterns in both January and July reveal oscillatory structures. Here are some trends:

* The average maximum temperature is higher in July (~25-27°C) than in January (~-1-1°C). Across all the locations in consideration, the range of average max temperature seems to be lesser (more consistent) in July than in January. There is one outlier in July (1988) where the average maximum temperature observed by a particular station seems to be much lesser (~14°C) than the recordings oberserved across other stations (between ~25-30°C). 
* For both Jan and July, rises and falls in average maximum temperature over the years seem to be following a somewhat predictable pattern that seems to be roughly repeating every 5 years.
* To generalize, we can observe a gradually upward rising trend over two-three years followed by a downward trend for the next two-three years. The ranges of these variations are changing from cycle to cycle but the structure of the pattern seems to remain somewhat constant. 



**Task 3.3**: Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r tmax vs tmin and snowfall plots}
tmax_vs_tmin = 
  ny_noaa_df %>% 
    ggplot(aes(x = tmin_c, y = tmax_c)) + 
    geom_hex() +
    scale_x_continuous(
      breaks = c(-60, -50, -40, -30, -20, -10, 0, 10, 20, 30, 40, 50, 60)) +
    labs(title = "Maximum Vs. Minimum Temperature",
       x = "Minimum Temperature (in °C)",
       y = "Maximum Temperature (in °C)",
       caption = "Composite: 1980 through 2010; across unique weather stations") +
    theme(
      legend.position = "left",
      plot.title = element_text(hjust = 0.5, face = "bold"),
      plot.caption = element_text(hjust = 0.5),
      axis.title = element_text(face = "bold"))

snow_fall_dist = 
  ny_noaa_df %>% 
    mutate(snow_inch = snow_inch * 25.4) %>% 
    rename(snow = snow_inch) %>%
  filter(snow > 0 & snow < 100) %>% 
  group_by(year, snow) %>% 
  summarise(year, snow) %>% 
    ggplot(aes(x = snow, y = year, group = year)) +
    geom_density_ridges(scale = .85) +
    scale_x_continuous(
      breaks = c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100)) +
    scale_y_continuous(
      breaks = c(1980, 1985, 1990, 1995, 2000, 2005, 2010)) +
    labs(title = "Snowfall Distribution by Year",
       x = "Snowfall (in millimeters (mm))",
       y = "Year",
       caption = "Distribution of snowfall values between 0 and 100 separately by year") +
    theme(
      plot.title = element_text(hjust = 0.5, face = "bold"),
      plot.caption = element_text(hjust = 0.5),
      axis.title = element_text(face = "bold"))

(tmax_vs_tmin + snow_fall_dist)
```


* 1) The most common tmax and tmin combination pairs are ~10-11°C (tmin) -- ~22-23°C (tmax) [~50000 counts], and ~-2-0°C (tmin) -- ~3-5°C (tmax) [~40000 counts]. 
* 2) Across all years, snowfall distribution seems to be following a somewhat bimodal pattern (ranging from 0-30 mm with modes around ~10 and ~25 mm) with two additional slight and slighter bumps (~50 mm and ~75 mm).
                                                              