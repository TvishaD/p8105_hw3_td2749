---
title: "p8105_hw3_td2749"
author: "Tvisha R. Devavarapu"
date: "2022-10-07"
output: github_document
---

```{r Setting up, message = FALSE}
library(tidyverse)
library(ggplot2)
library(ggridges)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 10,
  fig.asp = .6,
  out.width = "90%"
)
```

## Problem 2: Accelerometer Data


**Task 2.1**: Load, tidy, and otherwise wrangle the data. Your final dataset should include all originally observed variables and values; have useful variable names; include a weekday vs weekend variable; and encode data with reasonable variable classes. Describe the resulting dataset (e.g. what variables exist, how many observations, etc).

```{r accel_data set up}
accel_data = 
  read_csv("./data/accel_data.csv") %>%
  janitor::clean_names() %>%
  mutate(day_type = ifelse(day %in% c("Saturday","Sunday"), "weekend", "weekday")) %>% 
  mutate(day = fct_relevel(day, "Monday", "Tuesday", "Wednesday", "Thursday",
                                  "Friday", "Saturday", "Sunday")) %>%
  pivot_longer(activity_1:activity_1440, names_to = "minute_of_day",
               values_to = "activity_count") %>% 
  group_by(week, day) %>%
  mutate(total_activity_count = sum(activity_count)) %>% 
  ungroup() %>% 
  pivot_wider(names_from = minute_of_day, values_from = activity_count) %>% 
  select(week, day_id, day, day_type, total_activity_count, everything())
```
_**Description**_: Without losing any of the originally observed variables and values, the resultant `accel_data` df contains 35 unique rows of observations. Each row represents a day (1-35 days). The final df has 1445 columns of variables: `week` consists of values from 1-5 depicting the respective week of observation, `day_id` and `day` identify the unique days of observation (all 7 days of a week * 5 weeks = 35 days = 35 rows), `day_type` indicates whether a given day is a weekend or a weekday, `total_activity_count` represents the sums across each minute's activity counts over a given day (sum of 1140 (24*60) values), and `activity_1` to `activity_1440` represent the activity counts across each minute. 



**Task 2.2**: Traditional analyses of accelerometer data focus on the total activity over the day. Using your tidied dataset, aggregate across minutes to create a total activity variable for each day, and create a table showing these totals. Are any trends apparent?
```{r Total Activity - Table and Graph}
total_activity_table = 
  accel_data %>% 
  group_by(week, day) %>% 
  summarize(total_activity_count) %>% 
  pivot_wider(names_from = day, values_from = total_activity_count) %>% 
  knitr::kable(caption = "Daily Total Activity: Across 5 weeks")

total_activity_table

accel_data %>%
  ggplot(aes(x = day, y = total_activity_count, color = week, group = week)) +
  geom_point() +
  geom_line() +
  scale_colour_gradientn(colours = c("#88CCEE", "#CC6677", "#DDCC77", "#117733", "#332288")) +
  labs(title = "Total Activity Count Vs. Day: Across 5 Weeks",
       x = "Day",
       y = "Total Activity Count") +
  theme(plot.title = element_text(hjust = 0.5),
        legend.position = "bottom")
```
_**Description**_: From the table and graph generated above, here are some observed trends:

* Compared to other weeks, activity is relatively more consistent throughout week 3.
* Activity on Tuesday seems to be most consistent across all five weeks.
* Generally upward trending activity between Tuesday and Thursday of weeks 1, 2, and 3.
* Activity on Saturday of weeks 4 and 5 seems extremely low, even assuming lower than usual activity. This could be a result of the accelerometer not being worn on these days. 
* Across all weeks, activity was greater on Wednesday than on Tuesday and on Friday than on Thursday (apart from week 4). 
* Apart from week 4, Fridays seem to have consistently high total activities.

NOTE: I'm aware that `week` is in continual format (by virtue of being made up of numeric values.) I tried converting it into character (to use it as a categorical variable) but that's not letting me plot this graph. I will learn of a way to rectify this situation! 



**Task 2.3**: Accelerometer data allows the inspection activity over the course of the day. Make a single-panel plot that shows the 24-hour activity time courses for each day and use color to indicate day of the week. Describe in words any patterns or conclusions you can make based on this graph.
```{r 24 Hour Plot}
accel_data %>%
  pivot_longer(activity_1:activity_1440, names_to = "minute_of_day", 
               names_prefix = "activity_", values_to = "activity_count") %>% 
  mutate(minute_of_day = as.numeric(minute_of_day)) %>% 
  ggplot(aes(x = minute_of_day, y = activity_count, color = day)) +
  geom_point(alpha = 0.5) +
  scale_x_continuous(
    breaks = c(360, 720, 1080, 1440), 
    labels = c("6 AM", "12 PM", "6 PM", "12 AM")) +
  labs(title = "Activity Count Vs. Hour",
       x = "Hour",
       y = "Activity Count",
       caption = "Composite: Monday through Sunday; across 5 weeks") +
  theme(plot.title = element_text(hjust = 0.5),
        plot.caption = element_text(hjust = 0.5),
        legend.position = "bottom") +
  viridis::scale_color_viridis(name = "week", discrete = TRUE)
```
_**Description**_: From the plot above, some patterns/conclusions are:

* Sunday morning and Friday evenings generally reveal clusters of activity. 
* Thursday mornings around 7 AM. Consistent activity pattern?
* Saturdays (around 5 PM and 8 PM)
* Highest activity on Mondays usually between 7-9 PM. 
* Tuesdays are relatively the least active days for this individual. 
* Some sparse high activity on Wednesday evenings (7-10 PM), other than which Wednesdays were pretty sedentary. 




## Problem 3: NY NOAA Data

```{r Setting up ny_noaa}
library(p8105.datasets)
data("ny_noaa")
```

**Task 3**: The goal is to do some exploration of this dataset. To that end, write a short description of the dataset, noting the size and structure of the data, describing some key variables, and indicating the extent to which missing data is an issue.


_**Description**_: The original `ny_noaa` dataset consists of `r nrow(ny_noaa)` rows of observations containing weather information recorded in `r length(unique(ny_noaa$id))` unique New York weather stations. Each row depicts readings from a particular day and a particular location. There are `r ncol(ny_noaa)` number of columns: `r ncol(ny_noaa)`. 'id' (character) identifies the station of recording, 'date' (double) indicates the day of observation, 'prcp' (numeric) depicts precipitation in tenths of mm (millimeters), 'snow' (numeric) depicts snowfall in mm, 'snwd' (numeric) depicts snow depth in mm, and 'tmax' (character) and 'tmin' (character) depict the maximum and minimum temperature observed on that day in tenths of degrees Celcius (Â°C). 

Upon brief inspection of the dataset, it can be concluded that there is quite a lot of missing data. There are some rows where all weather recordings are missing, and some where there are certain combinations missing. An initial broad assumption for this could be that the missing values could reflect: 

a) No rain/snow/snowdepth to measure at a specific location on a certain day. (Weather conditions/seasonal shifts)
b) No temperature and/or snow fall readings at certain weather stations

As conditions about the availability of data are not explicit, it is not clear whether the rows with NA values should be completely discarded or imputed with some other values. This might change based on the needs of the questions being answered. If there is a lot of missing data that would otherwise be required to answer a specific/relevant question, this dataset would be considered less effective.

Note: There are no missing values in the `id` and `date` columns. 


**Task 3.1**: Do some data cleaning. Create separate variables for year, month, and day. Ensure observations for temperature, precipitation, and snowfall are given in reasonable units. For snowfall, what are the most commonly observed values? Why?
```{r relatively tidy ny_noaa_df}
ny_noaa_df = 
  ny_noaa %>%
    janitor::clean_names() %>% 
      mutate(date, date = as.character(date)) %>% 
        separate(date, into = c("year", "month", "day"), sep = "-") %>% 
    mutate(
      year = as.numeric(year),
      month = as.numeric(month),
      day = as.numeric(day),
      tmax, tmax = as.numeric(tmax),
      tmin = as.numeric(tmin),
      tmax = tmax/10,
      tmin = tmin/10,
      prcp = prcp/10,
      snow = snow/25.4) %>% 
    rename(
      prcp_mm = prcp, 
      snow_inch = snow, 
      snwd_mm = snwd, 
      tmax_c = tmax, 
      tmin_c = tmin)
```
For snowfall, the most commonly observed value is `r tail(names(sort(table(ny_noaa_df$snow_inch))), 1)`inches. There could be several reasons for this occurrence: 

* It only snows during few months of the year (November end to March beginning), explaining the abundance of '0' readings across the other months. 
* Even within those months, it might not be snowing everyday. As snowfall is measured to observe fresh-falling snow on a 24-hr basis, it is reasonable to infer that there may be certain non-snowing days during the winter months. 


**Task 3.2**: Make a two-panel plot showing the average max temperature in January and in July in each station across years. Is there any observable / interpretable structure? Any outliers?
```{r ny_noaa avg.tmax jan and july}
ny_noaa_df %>%
  group_by(id, year, month) %>%
  filter(month %in% c(1, 7)) %>% 
  summarize(mean_monthly_tmax = mean(tmax_c, na.rm = TRUE)) %>% 
  mutate(
    month = as.character(month),
    month = recode(month, "1" = "January", "7" = "July")) %>% 
  drop_na(mean_monthly_tmax) %>% 
  ggplot(aes(x = year, y = mean_monthly_tmax, group = id, color = id)) + 
    geom_point(alpha = .1) +
    geom_line(alpha = .3) + 
    scale_x_continuous(
      breaks = c(1980, 1985, 1990, 1995, 2000, 2005, 2010)) +
    facet_grid(. ~ month) +
    labs(title = "Average Monthly Maximum Temperature (Jan and July) Vs. Year",
       x = "Year",
       y = "Average Maximum Temperature (in Â°C)",
       caption = "Composite: 1980 through 2010; across unique weather stations") +
    theme(
      legend.position = "none",
      plot.title = element_text(hjust = 0.5, face = "bold"),
      plot.caption = element_text(hjust = 0.5),
      axis.title = element_text(face = "bold"))
```
**Description**: ADD!
...
...
...
...
...
...
...
...



**Task 3.3**: Make a two-panel plot showing (i) tmax vs tmin for the full dataset (note that a scatterplot may not be the best option); and (ii) make a plot showing the distribution of snowfall values greater than 0 and less than 100 separately by year.

```{r tmax vs tmin and snowfall plots}
tmax_vs_tmin = 
  ny_noaa_df %>% 
    ggplot(aes(x = tmin_c, y = tmax_c)) + 
    geom_hex() +
    scale_x_continuous(
      breaks = c(-60, -50, -40, -30, -20, -10, 0, 10, 20, 30, 40, 50, 60)) +
    labs(title = "Maximum Vs. Minimum Temperature",
       x = "Minimum Temperature (in Â°C)",
       y = "Maximum Temperature (in Â°C)",
       caption = "Composite: 1980 through 2010; across unique weather stations") +
    theme(
      legend.position = "none",
      plot.title = element_text(hjust = 0.5, face = "bold"),
      plot.caption = element_text(hjust = 0.5),
      axis.title = element_text(face = "bold"))

snow_fall_dist = 
  ny_noaa_df %>% 
    mutate(snow_inch = snow_inch * 25.4) %>% 
    rename(snow = snow_inch) %>%
  filter(snow > 0 & snow < 100) %>% 
  group_by(year, snow) %>% 
  summarise(year, snow) %>% 
    ggplot(aes(x = snow, y = year, group = year)) +
    geom_density_ridges(scale = .85) +
    scale_x_continuous(
      breaks = c(0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100)) +
    scale_y_continuous(
      breaks = c(1980, 1985, 1990, 1995, 2000, 2005, 2010)) +
    labs(title = "Snowfall Distribution by Year",
       x = "Snowfall (in millimeters (mm))",
       y = "Year",
       caption = "Distribution of snowfall values between 0 and 100 separately by year") +
    theme(
      legend.position = "none",
      plot.title = element_text(hjust = 0.5, face = "bold"),
      plot.caption = element_text(hjust = 0.5),
      axis.title = element_text(face = "bold"))

(tmax_vs_tmin + snow_fall_dist)
```


                                                              